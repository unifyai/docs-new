---
title: 'trigger_evaluation'
---

```python
def trigger_evaluation(
    evaluator: str,
    dataset: str,
    endpoint: str,
    client_side_scores: Optional[str] = None,
    api_key: Optional[str] = None,
) -> Dict[str, Any]:
```

<p align="right">[source code](https://github.com/unifyai/unify/tree/584195d96c7865135ab1579e408513263340600c/unify/utils/evaluations.py#L29)</p>

Trigger an evaluation for a specific dataset using a given evaluator and endpoint.

**Arguments**:

- `evaluator` - Name of the evaluator to use.
- `dataset` - Name of the uploaded dataset to evaluate.
- `endpoint` - Name of the endpoint to evaluate. Must be specified using the `model@provider` format.
- `client_side_scores` - Optional path to a JSONL file containing client-side scores.
- `api_key` - If specified, unify API key to be used. Defaults to the value in the `UNIFY_KEY` environment variable.



**Returns**:

A dictionary containing the response from the API.



**Raises**:

- `requests.HTTPError`: If the API request fails.
- `KeyError`: If the API key is not provided and not set in the environment.
- `FileNotFoundError`: If the client_side_scores file is specified but not found.