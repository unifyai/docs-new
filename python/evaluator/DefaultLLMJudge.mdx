---
title: 'DefaultLLMJudge'
---

```python
class DefaultLLMJudge
```

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L379)</p>



## properties

---

### class\_config

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L59)</p>

```python
def class_config(self) -> Dict[float, str]:
```



---

### client

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L256)</p>

```python
def client(self) -> Union[Unify, AsyncUnify]:
```



---

### include\_rationale

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L252)</p>

```python
def include_rationale(self) -> bool:
```



---

### name

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L46)</p>

```python
def name(self) -> Optional[str]:
```



---

### scorer

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L428)</p>

```python
def scorer(self) -> Type[DefaultJudgeScore]:
```



## setters

---

### set\_client

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L265)</p>

```python
def set_client(self, value: Union[Unify, AsyncUnify]) -> None:
```



---

### set\_include\_rationale

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L262)</p>

```python
def set_include_rationale(self, value: bool) -> None:
```



---

### set\_name

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L53)</p>

```python
def set_name(self, value: str):
```



## methods

---

### evaluate

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L115)</p>

```python
def evaluate(
            self,
            prompt: Union[str, Prompt],
            response: Union[ChatCompletion, str],
            agent: Union[str, _Client, Agent],
            **kwargs
    ) -> Union[Evaluation, EvaluationSet]:
```

Evaluate the given response for this input prompt, with optional extra data.

**Arguments**:

- `prompt` - The user message or the full input prompt being responded to.
- `response` - The response which is being evaluated, either as just the most
- `agent` - The agent that made the response, which is being evaluated.
- `kwargs` - Extra information relevant to the prompt, as is stored in the Datum.



**Returns**:

An Evaluation instance, containing the prompt, response, agent, score and
optional extra data used during the evaluation.

---

### from\_upstream

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L111)</p>

```python
def from_upstream() -> Evaluator:
```



---

### upload

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L108)</p>

```python
def upload(self, overwrite: bool = False) -> Self:
```



## dunder_methods

---

### \_\_init\_\_

<p align="right">[source code](https://github.com/unifyai/unify/tree/72e532ffdea1fc073200aba3b36b79a65e6b9651/unify/evaluator.py#L381)</p>

```python
def __init__(
            self,
            client: Union[Unify, AsyncUnify]
    ):
```

Create a default Judge, which uses a standard task-agnostic score and a generic
system prompt. This should judge work okay on a range of tasks, but the best
performance will be achieved by subclassing LLMJudge and creating your own.

**Arguments**:

- `client` - The client which holds the LLM used under the hood for judging.