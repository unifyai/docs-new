---
title: 'clients'
---

<a id="clients.Client"></a>

## Client

```python
class Client(ABC)
```

Base Abstract class for interacting with the Unify chat completions endpoint.

<a id="clients.Client.__init__"></a>

---

### \_\_init\_\_

```python
def __init__(endpoint: Optional[str] = None,
             model: Optional[str] = None,
             provider: Optional[str] = None,
             api_key: Optional[str] = None) -> None
```

Initialize the Unify client.

**Arguments**:

- `endpoint` _str, optional_ - Endpoint name in OpenAI API format:
  \<model_name\>@\<provider_name\>
  Defaults to None.
  
- `model` _str, optional_ - Name of the model.
  
- `provider` _str, optional_ - Name of the provider.
  
- `api_key` _str, optional_ - API key for accessing the Unify API.
  If None, it attempts to retrieve the API key from the
  environment variable UNIFY_KEY.
  Defaults to None.
  

**Raises**:

- `UnifyError` - If the API key is missing.

<a id="clients.Client.model"></a>

---

### model

```python
@property
def model() -> str
```

Get the model name.  

**Returns**:

- `str` - The model name.

<a id="clients.Client.set_model"></a>

---

### set\_model

```python
def set_model(value: str) -> None
```

Set the model name.  

**Arguments**:

- `value` _str_ - The model name.

<a id="clients.Client.provider"></a>

---

### provider

```python
@property
def provider() -> Optional[str]
```

Get the provider name.  

**Returns**:

- `str` - The provider name.

<a id="clients.Client.set_provider"></a>

---

### set\_provider

```python
def set_provider(value: str) -> None
```

Set the provider name.  

**Arguments**:

- `value` _str_ - The provider name.

<a id="clients.Client.endpoint"></a>

---

### endpoint

```python
@property
def endpoint() -> str
```

Get the endpoint name.  

**Returns**:

- `str` - The endpoint name.

<a id="clients.Client.set_endpoint"></a>

---

### set\_endpoint

```python
def set_endpoint(value: str) -> None
```

Set the endpoint name.  

**Arguments**:

- `value` _str_ - The endpoint name.

<a id="clients.Client.get_credit_balance"></a>

---

### get\_credit\_balance

```python
def get_credit_balance() -> float
```

Get the remaining credits left on your account.

**Returns**:

  int or None: The remaining credits on the account
  if successful, otherwise None.

**Raises**:

- `BadRequestError` - If there was an HTTP error.
- `ValueError` - If there was an error parsing the JSON response.

<a id="clients.Unify"></a>

## Unify

```python
class Unify(Client)
```

Class for interacting with the Unify chat completions endpoint in a synchronous manner.

<a id="clients.Unify.generate"></a>

---

### generate

```python
def generate(user_prompt: Optional[str] = None,
             system_prompt: Optional[str] = None,
             messages: Optional[List[Dict[str, str]]] = None,
             *,
             max_tokens: Optional[int] = 1024,
             stop: Optional[List[str]] = None,
             stream: bool = False,
             temperature: Optional[float] = 1.0,
             message_content_only: bool = True,
             **kwargs) -> Union[Generator[str, None, None], str]
```

Generate content using the Unify API.

**Arguments**:

- `user_prompt` _Optional[str]_ - A string containing the user prompt.
  If provided, messages must be None.
  
- `system_prompt` _Optional[str]_ - An optional string containing the
  system prompt.
  
- `messages` _List[Dict[str, str]]_ - A list of dictionaries containing the
  conversation history. If provided, user_prompt must be None.
  
- `max_tokens` _Optional[int]_ - The max number of output tokens.
  Defaults to the provider's default max_tokens when the value is None.
  
- `stop` _Optional[List[str]]_ - Up to 4 sequences where the API will stop generating further tokens.
  
- `stream` _bool_ - If True, generates content as a stream.
  If False, generates content as a single response.
  Defaults to False.
  
- `temperature` _Optional[float]_ - What sampling temperature to use, between 0 and 2.
  Higher values like 0.8 will make the output more random,
  while lower values like 0.2 will make it more focused and deterministic.
  Defaults to the provider's default max_tokens when the value is None.
  
- `message_content_only` _bool_ - If True, only return the message content
  chat_completion.choices[0].message.content.strip(" ") from the OpenAI return.
  Otherwise, the full response chat_completion is returned.
  Defaults to True.
  
- `kwargs` - Additional keyword arguments to be passed to the chat.completions.create() method
  of the openai.OpenAI() class, from the OpenAI Python client, which runs under the hood.
  

**Returns**:

  Union[Generator[str, None, None], str]: If stream is True,
  returns a generator yielding chunks of content.
  If stream is False, returns a single string response.
  

**Raises**:

- `UnifyError` - If an error occurs during content generation.

<a id="clients.AsyncUnify"></a>

## AsyncUnify

```python
class AsyncUnify(Client)
```

Class for interacting with the Unify chat completions endpoint in a synchronous manner.

<a id="clients.AsyncUnify.generate"></a>

---

### generate

```python
async def generate(user_prompt: Optional[str] = None,
                   system_prompt: Optional[str] = None,
                   messages: Optional[List[Dict[str, str]]] = None,
                   *,
                   max_tokens: Optional[int] = None,
                   stop: Optional[List[str]] = None,
                   stream: bool = False,
                   temperature: Optional[float] = 1.0,
                   message_content_only: bool = True,
                   **kwargs) -> Union[AsyncGenerator[str, None], str]
```

Generate content asynchronously using the Unify API.

**Arguments**:

- `user_prompt` _Optional[str]_ - A string containing the user prompt.
  If provided, messages must be None.
  
- `system_prompt` _Optional[str]_ - An optional string containing the
  system prompt.
  
- `messages` _List[Dict[str, str]]_ - A list of dictionaries containing the
  conversation history. If provided, user_prompt must be None.
  
- `max_tokens` _Optional[int]_ - The max number of output tokens, defaults
  to the provider's default max_tokens when the value is None.
  
- `stop` _Optional[List[str]]_ - Up to 4 sequences where the API will stop generating further tokens.
  
- `stream` _bool_ - If True, generates content as a stream.
  If False, generates content as a single response.
  Defaults to False.
  
- `temperature` _Optional[float]_ - What sampling temperature to use, between 0 and 2.
  Higher values like 0.8 will make the output more random,
  while lower values like 0.2 will make it more focused and deterministic.
  Defaults to the provider's default max_tokens when the value is None.
  
- `message_content_only` _bool_ - If True, only return the message content
  chat_completion.choices[0].message.content.strip(" ") from the OpenAI return.
  Otherwise, the full response chat_completion is returned.
  Defaults to True.
  
- `kwargs` - Additional keyword arguments to be passed to the chat.completions.create() method
  of the openai.OpenAI() class, from the OpenAI Python client, which runs under the hood.
  

**Returns**:

  Union[AsyncGenerator[str, None], List[str]]: If stream is True,
  returns an asynchronous generator yielding chunks of content.
  If stream is False, returns a list of string responses.
  

**Raises**:

- `UnifyError` - If an error occurs during content generation.

<a id="chat"></a>
