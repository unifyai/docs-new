---
title: 'dataset'
---

<a id="dataset.Dataset"></a>

## Dataset

```python
class Dataset()
```

<a id="dataset.Dataset.__init__"></a>

---

### \_\_init\_\_

```python
def __init__(data: Union[str, List[Union[str, Dict, Prompt, DatasetEntry]]],
             *,
             name: str = None,
             auto_sync: Union[bool, str] = False,
             api_key: Optional[str] = None)
```

Initialize a local dataset of LLM queries.

**Arguments**:

- `data` - The data for populating the dataset. This can either can a string
  specifying an upstream dataset, a list of user messages, a list of full
  queries, or a list of dicts of queries alongside any extra fields.
  
- `name` - The name of the dataset.
  
- `auto_sync` - Whether to automatically keep this dataset fully synchronized
  with the upstream variant at all times. If `True` or "both" then the sync
  will be bi-directional, if "upload_only" then all local changes will be
  uploaded to the upstream account without any downloads, if "download_only"
  then all upstream changes will be downloaded locally without any uploads.
  If `False` or "neither" then no synchronization will be done automatically.
  
- `api_key` - API key for accessing the Unify API. If None, it attempts to
  retrieve the API key from the environment variable UNIFY_KEY. Defaults to
  None.
  

**Raises**:

- `UnifyError` - If the API key is missing.

<a id="dataset.Dataset.from_upstream"></a>

---

### from\_upstream

```python
@staticmethod
def from_upstream(name: str,
                  auto_sync: bool = False,
                  api_key: Optional[str] = None)
```

Initialize a local dataset of LLM queries, from the upstream dataset.

**Arguments**:

- `name` - The name of the dataset.
  
- `auto_sync` - Whether to automatically keep this dataset fully synchronized
  with the upstream variant at all times.
  
- `api_key` - API key for accessing the Unify API. If None, it attempts to
  retrieve the API key from the environment variable UNIFY_KEY. Defaults to
  None.
  

**Raises**:

- `UnifyError` - If the API key is missing.

<a id="dataset.Dataset.from_file"></a>

---

### from\_file

```python
@staticmethod
def from_file(filepath: str,
              name: str = None,
              auto_sync: bool = False,
              api_key: Optional[str] = None)
```

Loads the dataset from a local .jsonl filepath.

**Arguments**:

- `filepath` - Filepath (.jsonl) to load the dataset from.
  
- `name` - The name of the dataset.
  
- `auto_sync` - Whether to automatically keep this dataset fully synchronized
  with the upstream variant at all times.
  
- `api_key` - API key for accessing the Unify API. If None, it attempts to
  retrieve the API key from the environment variable UNIFY_KEY. Defaults to
  None.

<a id="dataset.Dataset.upload"></a>

---

### upload

```python
def upload(overwrite=False)
```

Uploads all unique local data in the dataset to the user account upstream.
This function will not download any uniques from upstream.
Use `sync` to synchronize and superset the datasets in both directions.
Set `overwrite=True` to disregard any pre-existing upstream data.

**Arguments**:

- `overwrite` - Whether to overwrite the upstream dataset if it already exists

<a id="dataset.Dataset.download"></a>

---

### download

```python
def download(overwrite=False)
```

Downloads all unique upstream data from the user account to the local dataset.
This function will not upload any unique values stored locally.
Use `sync` to synchronize and superset the datasets in both directions.
Set `overwrite=True` to disregard any pre-existing data stored in this class.

**Arguments**:

- `overwrite` - Whether to overwrite the local data, if any already exists

<a id="dataset.Dataset.sync"></a>

---

### sync

```python
def sync()
```

Synchronize the dataset in both directions, downloading any values missing
locally, and uploading any values missing from upstream in the account.

<a id="dataset.Dataset.upstream_diff"></a>

---

### upstream\_diff

```python
def upstream_diff()
```

Prints the difference between the local dataset and the upstream dataset.

<a id="dataset.Dataset.save_to_file"></a>

---

### save\_to\_file

```python
def save_to_file(filepath: str)
```

Saves to dataset to a local .jsonl filepath.

**Arguments**:

- `filepath` - Filepath (.jsonl) to save the dataset to.

<a id="dataset.Dataset.add"></a>

---

### add

```python
def add(other: Dataset)
```

Adds another dataset to this one, return a new Dataset instance, with this
new dataset receiving all unique queries from the other added dataset.

**Arguments**:

- `other` - The other dataset being added to this one.

<a id="dataset.Dataset.sub"></a>

---

### sub

```python
def sub(other: Dataset)
```

Subtracts another dataset from this one, return a new Dataset instance, with
this new dataset losing all queries from the other subtracted dataset.

**Arguments**:

- `other` - The other dataset being added to this one.

<a id="dataset.Dataset.__iadd__"></a>

---

### \_\_iadd\_\_

```python
def __iadd__(other)
```

Adds another dataset to this one, with this dataset receiving all unique queries
from the other added dataset.

**Arguments**:

- `other` - The other dataset being added to this one.

<a id="dataset.Dataset.__isub__"></a>

---

### \_\_isub\_\_

```python
def __isub__(other)
```

Subtracts another dataset from this one, with this dataset losing all queries
from the other subtracted dataset.

**Arguments**:

- `other` - The other dataset being added to this one.

<a id="types"></a>
