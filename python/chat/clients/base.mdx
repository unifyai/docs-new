---
title: 'clients.base'
---

<a id="chat.clients.base.Client"></a>

## Client

```python
class Client(ABC)
```

Base Abstract class for interacting with the Unify chat completions endpoint.

<a id="chat.clients.base.Client.__init__"></a>

---

### \_\_init\_\_

```python
def __init__(*,
             system_message: Optional[str] = None,
             messages: Optional[Iterable[ChatCompletionMessageParam]] = None,
             frequency_penalty: Optional[float] = None,
             logit_bias: Optional[Dict[str, int]] = None,
             logprobs: Optional[bool] = None,
             top_logprobs: Optional[int] = None,
             max_tokens: Optional[int] = 1024,
             n: Optional[int] = None,
             presence_penalty: Optional[float] = None,
             response_format: Optional[ResponseFormat] = None,
             seed: Optional[int] = None,
             stop: Union[Optional[str], List[str]] = None,
             stream: Optional[bool] = False,
             stream_options: Optional[ChatCompletionStreamOptionsParam] = None,
             temperature: Optional[float] = 1.0,
             top_p: Optional[float] = None,
             tools: Optional[Iterable[ChatCompletionToolParam]] = None,
             tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,
             parallel_tool_calls: Optional[bool] = None,
             use_custom_keys: bool = False,
             tags: Optional[List[str]] = None,
             api_key: Optional[str] = None,
             message_content_only: bool = True,
             cache: bool = False,
             extra_headers: Optional[Headers] = None,
             extra_query: Optional[Query] = None,
             **kwargs) -> None
```

Initialize the base Unify client.

**Arguments**:

- `system_message` - An optional string containing the system message.
  
- `messages` - A list of messages comprising the conversation so far.
  If provided, user_message must be None.
  
- `frequency_penalty` - Number between -2.0 and 2.0. Positive values penalize new
  tokens based on their existing frequency in the text so far, decreasing the
  model's likelihood to repeat the same line verbatim.
  
- `logit_bias` - Modify the likelihood of specified tokens appearing in the
  completion. Accepts a JSON object that maps tokens (specified by their token
  ID in the tokenizer) to an associated bias value from -100 to 100.
  Mathematically, the bias is added to the logits generated by the model prior
  to sampling. The exact effect will vary per model, but values between -1 and
  1 should decrease or increase likelihood of selection; values like -100 or
  100 should result in a ban or exclusive selection of the relevant token.
  
- `logprobs` - Whether to return log probabilities of the output tokens or not.
  If true, returns the log probabilities of each output token returned in the
  content of message.
  
- `top_logprobs` - An integer between 0 and 20 specifying the number of most
  likely tokens to return at each token position, each with an associated log
  probability. logprobs must be set to true if this parameter is used.
  
- `max_tokens` - The maximum number of tokens that can be generated in the chat
  completion. The total length of input tokens and generated tokens is limited
  by the model's context length. Defaults to the provider's default max_tokens
  when the value is None.
  
- `n` - How many chat completion choices to generate for each input message. Note
  that you will be charged based on the number of generated tokens across all
  of the choices. Keep n as 1 to minimize costs.
  
- `presence_penalty` - Number between -2.0 and 2.0. Positive values penalize new
  tokens based on whether they appear in the text so far, increasing the
  model's likelihood to talk about new topics.
  
- `response_format` - An object specifying the format that the model must output.
  Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
  Structured Outputs which ensures the model will match your supplied JSON
  schema. Learn more in the Structured Outputs guide. Setting to
  `{ "type": "json_object" }` enables JSON mode, which ensures the message the
  model generates is valid JSON.
  
- `seed` - If specified, a best effort attempt is made to sample
  deterministically, such that repeated requests with the same seed and
  parameters should return the same result. Determinism is not guaranteed, and
  you should refer to the system_fingerprint response parameter to monitor
  changes in the backend.
  
- `stop` - Up to 4 sequences where the API will stop generating further tokens.
  
- `stream` - If True, generates content as a stream. If False, generates content
  as a single response. Defaults to False.
  
- `stream_options` - Options for streaming response. Only set this when you set
- `stream` - true.
  
- `temperature` - What sampling temperature to use, between 0 and 2.
  Higher values like 0.8 will make the output more random,
  while lower values like 0.2 will make it more focused and deterministic.
  It is generally recommended to alter this or top_p, but not both.
  Defaults to the provider's default max_tokens when the value is None.
  
- `top_p` - An alternative to sampling with temperature, called nucleus sampling,
  where the model considers the results of the tokens with top_p probability
  mass. So 0.1 means only the tokens comprising the top 10% probability mass
  are considered. Generally recommended to alter this or temperature, but not
  both.
  
- `tools` - A list of tools the model may call. Currently, only functions are
  supported as a tool. Use this to provide a list of functions the model may
  generate JSON inputs for. A max of 128 functions are supported.
  
- `tool_choice` - Controls which (if any) tool is called by the
  model. none means the model will not call any tool and instead generates a
  message. auto means the model can pick between generating a message or
  calling one or more tools. required means the model must call one or more
  tools. Specifying a particular tool via
  `{ "type": "function", "function": {"name": "my_function"} }`
  forces the model to call that tool.
  none is the default when no tools are present. auto is the default if tools
  are present.
  
- `parallel_tool_calls` - Whether to enable parallel function calling during tool
  use.
  
- `use_custom_keys` - Whether to use custom API keys or our unified API keys
  with the backend provider.
  
- `tags` - Arbitrary number of tags to classify this API query as needed. Helpful
  for generally grouping queries across tasks and users, for logging purposes.
  
- `message_content_only` - If True, only return the message content
  chat_completion.choices[0].message.content.strip(" ") from the OpenAI
  return. Otherwise, the full response chat_completion is returned.
  Defaults to True.
  
- `cache` - If True, then the arguments will be stored in a local cache file, and
  any future calls with identical arguments will read from the cache instead
  of running the LLM query. This can help to save costs and also debug
  multi-step LLM applications, while keeping early steps fixed.
  This argument only has any effect when stream=False.
  
- `extra_headers` - Additional "passthrough" headers for the request which are
  provider-specific, and are not part of the OpenAI standard. They are handled
  by the provider-specific API.
  
- `extra_query` - Additional "passthrough" query parameters for the request which
  are provider-specific, and are not part of the OpenAI standard. They are
  handled by the provider-specific API.
  
- `kwargs` - Additional "passthrough" JSON properties for the body of the
  request, which are provider-specific, and are not part of the OpenAI
  standard. They will be handled by the provider-specific API.
  

**Raises**:

- `UnifyError` - If the API key is missing.

<a id="chat.clients.base.Client.system_message"></a>

---

### system\_message

```python
@property
def system_message() -> Optional[str]
```

Get the default system message, if set.

**Returns**:

  The default system message.

<a id="chat.clients.base.Client.messages"></a>

---

### messages

```python
@property
def messages() -> Optional[Iterable[ChatCompletionMessageParam]]
```

Get the default messages, if set.

**Returns**:

  The default messages.

<a id="chat.clients.base.Client.frequency_penalty"></a>

---

### frequency\_penalty

```python
@property
def frequency_penalty() -> Optional[float]
```

Get the default frequency penalty, if set.

**Returns**:

  The default frequency penalty.

<a id="chat.clients.base.Client.logit_bias"></a>

---

### logit\_bias

```python
@property
def logit_bias() -> Optional[Dict[str, int]]
```

Get the default logit bias, if set.

**Returns**:

  The default logit bias.

<a id="chat.clients.base.Client.logprobs"></a>

---

### logprobs

```python
@property
def logprobs() -> Optional[bool]
```

Get the default logprobs, if set.

**Returns**:

  The default logprobs.

<a id="chat.clients.base.Client.top_logprobs"></a>

---

### top\_logprobs

```python
@property
def top_logprobs() -> Optional[int]
```

Get the default top logprobs, if set.

**Returns**:

  The default top logprobs.

<a id="chat.clients.base.Client.max_tokens"></a>

---

### max\_tokens

```python
@property
def max_tokens() -> Optional[int]
```

Get the default max tokens, if set.

**Returns**:

  The default max tokens.

<a id="chat.clients.base.Client.n"></a>

---

### n

```python
@property
def n() -> Optional[int]
```

Get the default n, if set.

**Returns**:

  The default n value.

<a id="chat.clients.base.Client.presence_penalty"></a>

---

### presence\_penalty

```python
@property
def presence_penalty() -> Optional[float]
```

Get the default presence penalty, if set.

**Returns**:

  The default presence penalty.

<a id="chat.clients.base.Client.response_format"></a>

---

### response\_format

```python
@property
def response_format() -> Optional[ResponseFormat]
```

Get the default response format, if set.

**Returns**:

  The default response format.

<a id="chat.clients.base.Client.seed"></a>

---

### seed

```python
@property
def seed() -> Optional[int]
```

Get the default seed value, if set.

**Returns**:

  The default seed value.

<a id="chat.clients.base.Client.stop"></a>

---

### stop

```python
@property
def stop() -> Union[Optional[str], List[str]]
```

Get the default stop value, if set.

**Returns**:

  The default stop value.

<a id="chat.clients.base.Client.stream"></a>

---

### stream

```python
@property
def stream() -> Optional[bool]
```

Get the default stream bool, if set.

**Returns**:

  The default stream bool.

<a id="chat.clients.base.Client.stream_options"></a>

---

### stream\_options

```python
@property
def stream_options() -> Optional[ChatCompletionStreamOptionsParam]
```

Get the default stream options, if set.

**Returns**:

  The default stream options.

<a id="chat.clients.base.Client.temperature"></a>

---

### temperature

```python
@property
def temperature() -> Optional[float]
```

Get the default temperature, if set.

**Returns**:

  The default temperature.

<a id="chat.clients.base.Client.top_p"></a>

---

### top\_p

```python
@property
def top_p() -> Optional[float]
```

Get the default top p value, if set.

**Returns**:

  The default top p value.

<a id="chat.clients.base.Client.tools"></a>

---

### tools

```python
@property
def tools() -> Optional[Iterable[ChatCompletionToolParam]]
```

Get the default tools, if set.

**Returns**:

  The default tools.

<a id="chat.clients.base.Client.tool_choice"></a>

---

### tool\_choice

```python
@property
def tool_choice() -> Optional[ChatCompletionToolChoiceOptionParam]
```

Get the default tool choice, if set.

**Returns**:

  The default tool choice.

<a id="chat.clients.base.Client.parallel_tool_calls"></a>

---

### parallel\_tool\_calls

```python
@property
def parallel_tool_calls() -> Optional[bool]
```

Get the default parallel tool calls bool, if set.

**Returns**:

  The default parallel tool calls bool.

<a id="chat.clients.base.Client.use_custom_keys"></a>

---

### use\_custom\_keys

```python
@property
def use_custom_keys() -> bool
```

Get the default use custom keys bool, if set.

**Returns**:

  The default use custom keys bool.

<a id="chat.clients.base.Client.tags"></a>

---

### tags

```python
@property
def tags() -> Optional[List[str]]
```

Get the default tags, if set.

**Returns**:

  The default tags.

<a id="chat.clients.base.Client.message_content_only"></a>

---

### message\_content\_only

```python
@property
def message_content_only() -> bool
```

Get the default message content only bool.

**Returns**:

  The default message content only bool.

<a id="chat.clients.base.Client.cache"></a>

---

### cache

```python
@property
def cache() -> bool
```

Get default the cache bool.

**Returns**:

  The default cache bool.

<a id="chat.clients.base.Client.extra_headers"></a>

---

### extra\_headers

```python
@property
def extra_headers() -> Optional[Headers]
```

Get the default extra headers, if set.

**Returns**:

  The default extra headers.

<a id="chat.clients.base.Client.extra_query"></a>

---

### extra\_query

```python
@property
def extra_query() -> Optional[Query]
```

Get the default extra query, if set.

**Returns**:

  The default extra query.

<a id="chat.clients.base.Client.extra_body"></a>

---

### extra\_body

```python
@property
def extra_body() -> Optional[Mapping[str, str]]
```

Get the default extra body, if set.

**Returns**:

  The default extra body.

<a id="chat.clients.base.Client.set_system_message"></a>

---

### set\_system\_message

```python
def set_system_message(value: str) -> None
```

Set the default system message.  

**Arguments**:

- `value` - The default system message.

<a id="chat.clients.base.Client.set_messages"></a>

---

### set\_messages

```python
def set_messages(value: Iterable[ChatCompletionMessageParam]) -> None
```

Set the default messages.  

**Arguments**:

- `value` - The default messages.

<a id="chat.clients.base.Client.set_frequency_penalty"></a>

---

### set\_frequency\_penalty

```python
def set_frequency_penalty(value: float) -> None
```

Set the default frequency penalty.  

**Arguments**:

- `value` - The default frequency penalty.

<a id="chat.clients.base.Client.set_logit_bias"></a>

---

### set\_logit\_bias

```python
def set_logit_bias(value: Dict[str, int]) -> None
```

Set the default logit bias.  

**Arguments**:

- `value` - The default logit bias.

<a id="chat.clients.base.Client.set_logprobs"></a>

---

### set\_logprobs

```python
def set_logprobs(value: bool) -> None
```

Set the default logprobs.  

**Arguments**:

- `value` - The default logprobs.

<a id="chat.clients.base.Client.set_top_logprobs"></a>

---

### set\_top\_logprobs

```python
def set_top_logprobs(value: int) -> None
```

Set the default top logprobs.  

**Arguments**:

- `value` - The default top logprobs.

<a id="chat.clients.base.Client.set_max_tokens"></a>

---

### set\_max\_tokens

```python
def set_max_tokens(value: int) -> None
```

Set the default max tokens.  

**Arguments**:

- `value` - The default max tokens.

<a id="chat.clients.base.Client.set_n"></a>

---

### set\_n

```python
def set_n(value: int) -> None
```

Set the default n value.  

**Arguments**:

- `value` - The default n value.

<a id="chat.clients.base.Client.set_presence_penalty"></a>

---

### set\_presence\_penalty

```python
def set_presence_penalty(value: float) -> None
```

Set the default presence penalty.  

**Arguments**:

- `value` - The default presence penalty.

<a id="chat.clients.base.Client.set_response_format"></a>

---

### set\_response\_format

```python
def set_response_format(value: ResponseFormat) -> None
```

Set the default response format.  

**Arguments**:

- `value` - The default response format.

<a id="chat.clients.base.Client.set_seed"></a>

---

### set\_seed

```python
def set_seed(value: int) -> None
```

Set the default seed value.  

**Arguments**:

- `value` - The default seed value.

<a id="chat.clients.base.Client.set_stop"></a>

---

### set\_stop

```python
def set_stop(value: Union[str, List[str]]) -> None
```

Set the default stop value.  

**Arguments**:

- `value` - The default stop value.

<a id="chat.clients.base.Client.set_stream"></a>

---

### set\_stream

```python
def set_stream(value: bool) -> None
```

Set the default stream bool.  

**Arguments**:

- `value` - The default stream bool.

<a id="chat.clients.base.Client.set_stream_options"></a>

---

### set\_stream\_options

```python
def set_stream_options(value: ChatCompletionStreamOptionsParam) -> None
```

Set the default stream options.  

**Arguments**:

- `value` - The default stream options.

<a id="chat.clients.base.Client.set_temperature"></a>

---

### set\_temperature

```python
def set_temperature(value: float) -> None
```

Set the default temperature.  

**Arguments**:

- `value` - The default temperature.

<a id="chat.clients.base.Client.set_top_p"></a>

---

### set\_top\_p

```python
def set_top_p(value: float) -> None
```

Set the default top p value.  

**Arguments**:

- `value` - The default top p value.

<a id="chat.clients.base.Client.set_tools"></a>

---

### set\_tools

```python
def set_tools(value: Iterable[ChatCompletionToolParam]) -> None
```

Set the default tools.  

**Arguments**:

- `value` - The default tools.

<a id="chat.clients.base.Client.set_tool_choice"></a>

---

### set\_tool\_choice

```python
def set_tool_choice(value: ChatCompletionToolChoiceOptionParam) -> None
```

Set the default tool choice.  

**Arguments**:

- `value` - The default tool choice.

<a id="chat.clients.base.Client.set_parallel_tool_calls"></a>

---

### set\_parallel\_tool\_calls

```python
def set_parallel_tool_calls(value: bool) -> None
```

Set the default parallel tool calls bool.  

**Arguments**:

- `value` - The default parallel tool calls bool.

<a id="chat.clients.base.Client.set_use_custom_keys"></a>

---

### set\_use\_custom\_keys

```python
def set_use_custom_keys(value: bool) -> None
```

Set the default use custom keys bool.  

**Arguments**:

- `value` - The default use custom keys bool.

<a id="chat.clients.base.Client.set_tags"></a>

---

### set\_tags

```python
def set_tags(value: List[str]) -> None
```

Set the default tags.  

**Arguments**:

- `value` - The default tags.

<a id="chat.clients.base.Client.set_message_content_only"></a>

---

### set\_message\_content\_only

```python
def set_message_content_only(value: bool) -> None
```

Set the default message content only bool.  

**Arguments**:

- `value` - The default message content only bool.

<a id="chat.clients.base.Client.set_cache"></a>

---

### set\_cache

```python
def set_cache(value: bool) -> None
```

Set the default cache bool.  

**Arguments**:

- `value` - The default cache bool.

<a id="chat.clients.base.Client.set_extra_headers"></a>

---

### set\_extra\_headers

```python
def set_extra_headers(value: Headers) -> None
```

Set the default extra headers.  

**Arguments**:

- `value` - The default extra headers.

<a id="chat.clients.base.Client.set_extra_query"></a>

---

### set\_extra\_query

```python
def set_extra_query(value: Query) -> None
```

Set the default extra query.  

**Arguments**:

- `value` - The default extra query.

<a id="chat.clients.base.Client.set_extra_body"></a>

---

### set\_extra\_body

```python
def set_extra_body(value: Body) -> None
```

Set the default extra body.  

**Arguments**:

- `value` - The default extra body.

<a id="chat.clients.base.Client.generate"></a>

---

### generate

```python
def generate(user_message: Optional[str] = None,
             system_message: Optional[str] = None,
             messages: Optional[Iterable[ChatCompletionMessageParam]] = None,
             *,
             frequency_penalty: Optional[float] = None,
             logit_bias: Optional[Dict[str, int]] = None,
             logprobs: Optional[bool] = None,
             top_logprobs: Optional[int] = None,
             max_tokens: Optional[int] = 1024,
             n: Optional[int] = None,
             presence_penalty: Optional[float] = None,
             response_format: Optional[ResponseFormat] = None,
             seed: Optional[int] = None,
             stop: Union[Optional[str], List[str]] = None,
             stream: Optional[bool] = False,
             stream_options: Optional[ChatCompletionStreamOptionsParam] = None,
             temperature: Optional[float] = 1.0,
             top_p: Optional[float] = None,
             tools: Optional[Iterable[ChatCompletionToolParam]] = None,
             tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,
             parallel_tool_calls: Optional[bool] = None,
             use_custom_keys: bool = False,
             tags: Optional[List[str]] = None,
             message_content_only: bool = True,
             cache: bool = False,
             extra_headers: Optional[Headers] = None,
             extra_query: Optional[Query] = None,
             **kwargs)
```

Generate content using the Unify API.

**Arguments**:

- `user_message` - A string containing the user message.
  If provided, messages must be None.
  
- `system_message` - An optional string containing the system message.
  
- `messages` - A list of messages comprising the conversation so far.
  If provided, user_message must be None.
  
- `frequency_penalty` - Number between -2.0 and 2.0. Positive values penalize new
  tokens based on their existing frequency in the text so far, decreasing the
  model's likelihood to repeat the same line verbatim.
  
- `logit_bias` - Modify the likelihood of specified tokens appearing in the
  completion. Accepts a JSON object that maps tokens (specified by their token
  ID in the tokenizer) to an associated bias value from -100 to 100.
  Mathematically, the bias is added to the logits generated by the model prior
  to sampling. The exact effect will vary per model, but values between -1 and
  1 should decrease or increase likelihood of selection; values like -100 or
  100 should result in a ban or exclusive selection of the relevant token.
  
- `logprobs` - Whether to return log probabilities of the output tokens or not.
  If true, returns the log probabilities of each output token returned in the
  content of message.
  
- `top_logprobs` - An integer between 0 and 20 specifying the number of most
  likely tokens to return at each token position, each with an associated log
  probability. logprobs must be set to true if this parameter is used.
  
- `max_tokens` - The maximum number of tokens that can be generated in the chat
  completion. The total length of input tokens and generated tokens is limited
  by the model's context length. Defaults to the provider's default max_tokens
  when the value is None.
  
- `n` - How many chat completion choices to generate for each input message. Note
  that you will be charged based on the number of generated tokens across all
  of the choices. Keep n as 1 to minimize costs.
  
- `presence_penalty` - Number between -2.0 and 2.0. Positive values penalize new
  tokens based on whether they appear in the text so far, increasing the
  model's likelihood to talk about new topics.
  
- `response_format` - An object specifying the format that the model must output.
  Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
  Structured Outputs which ensures the model will match your supplied JSON
  schema. Learn more in the Structured Outputs guide. Setting to
  `{ "type": "json_object" }` enables JSON mode, which ensures the message the
  model generates is valid JSON.
  
- `seed` - If specified, a best effort attempt is made to sample
  deterministically, such that repeated requests with the same seed and
  parameters should return the same result. Determinism is not guaranteed, and
  you should refer to the system_fingerprint response parameter to monitor
  changes in the backend.
  
- `stop` - Up to 4 sequences where the API will stop generating further tokens.
  
- `stream` - If True, generates content as a stream. If False, generates content
  as a single response. Defaults to False.
  
- `stream_options` - Options for streaming response. Only set this when you set
- `stream` - true.
  
- `temperature` - What sampling temperature to use, between 0 and 2.
  Higher values like 0.8 will make the output more random,
  while lower values like 0.2 will make it more focused and deterministic.
  It is generally recommended to alter this or top_p, but not both.
  Defaults to the provider's default max_tokens when the value is None.
  
- `top_p` - An alternative to sampling with temperature, called nucleus sampling,
  where the model considers the results of the tokens with top_p probability
  mass. So 0.1 means only the tokens comprising the top 10% probability mass
  are considered. Generally recommended to alter this or temperature, but not
  both.
  
- `tools` - A list of tools the model may call. Currently, only functions are
  supported as a tool. Use this to provide a list of functions the model may
  generate JSON inputs for. A max of 128 functions are supported.
  
- `tool_choice` - Controls which (if any) tool is called by the
  model. none means the model will not call any tool and instead generates a
  message. auto means the model can pick between generating a message or
  calling one or more tools. required means the model must call one or more
  tools. Specifying a particular tool via
  `{ "type": "function", "function": {"name": "my_function"} }`
  forces the model to call that tool.
  none is the default when no tools are present. auto is the default if tools
  are present.
  
- `parallel_tool_calls` - Whether to enable parallel function calling during tool
  use.
  
- `use_custom_keys` - Whether to use custom API keys or our unified API keys
  with the backend provider.
  
- `tags` - Arbitrary number of tags to classify this API query as needed. Helpful
  for generally grouping queries across tasks and users, for logging purposes.
  
- `message_content_only` - If True, only return the message content
  chat_completion.choices[0].message.content.strip(" ") from the OpenAI
  return. Otherwise, the full response chat_completion is returned.
  Defaults to True.
  
- `cache` - If True, then the arguments will be stored in a local cache file, and
  any future calls with identical arguments will read from the cache instead
  of running the LLM query. This can help to save costs and also debug
  multi-step LLM applications, while keeping early steps fixed.
  This argument only has any effect when stream=False.
  
- `extra_headers` - Additional "passthrough" headers for the request which are
  provider-specific, and are not part of the OpenAI standard. They are handled
  by the provider-specific API.
  
- `extra_query` - Additional "passthrough" query parameters for the request which
  are provider-specific, and are not part of the OpenAI standard. They are
  handled by the provider-specific API.
  
- `kwargs` - Additional "passthrough" JSON properties for the body of the
  request, which are provider-specific, and are not part of the OpenAI
  standard. They will be handled by the provider-specific API.
  

**Returns**:

  If stream is True, returns a generator yielding chunks of content.
  If stream is False, returns a single string response.
  

**Raises**:

- `UnifyError` - If an error occurs during content generation.

<a id="chat.clients.base.Client.get_credit_balance"></a>

---

### get\_credit\_balance

```python
def get_credit_balance() -> Union[float, None]
```

Get the remaining credits left on your account.

**Returns**:

  The remaining credits on the account if successful, otherwise None.

**Raises**:

- `BadRequestError` - If there was an HTTP error.
- `ValueError` - If there was an error parsing the JSON response.

