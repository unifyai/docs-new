---
title: 'Arguments'
---

## Introduction

With so many LLMs and providers constantly coming onto the scene, each of these is
increasingly striving to provide unique value to end users, and this means that there
are often diverging features offered behind the API.

For example, some models support RAG directly in the API,
others support function calling, tool use, image processing, audio,
structured output (such as json mode),
and many other increasingly complex modes of operation.

## Unified Arguments

Our API builds on top of and extends LiteLLM under the hood. As a starting point, we recommend you go through the
[Input Params](https://docs.litellm.ai/docs/completion/input) in their
chat completions [docs](https://docs.litellm.ai/docs/completion) to find out the *unified* arguments supported.
We use the latest PyPI version at all times. In general, all models and providers are unified under the
[OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat),
as can be seen in our own chat completion
[API reference](https://docs.unify.ai/api-reference/llm_queries/chat_completions).
We also extend the LiteLLM backend to support several other providers, such as [Lepton AI](https://www.lepton.ai/)
and [OctoAI](https://octo.ai/).

### Tool Use Example

OpenAI and Anthropic have different interfaces for tool use.
Since our API adheres to the OpenAI standard, we accept tools as specified by this standard.

This is the default function calling example from OpenAI, working with an Anthropic model:

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
            },
        },
    }
]

client = unify.Unify("claude-3.5-sonnet@aws-bedrock")
client.generate("What is the current temperature of New York, San Francisco and Chicago?", tools=tools, tool_choice="auto", message_content_only=False)
```

### Vision Example

Unify also supports multi-modal inputs. Below are a couple of examples analyzing the content of images.

Firstly, let's use `gpt-4o` to work out what's in this picture:
<p align="left">
    <img width={384} src="/images/nature.png" alt="Nature Image" />
</p>

```python
import unify
client = unify.Unify("gpt-4o@openai")
response = client.generate(
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What’s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response)
```

We get something like the following:

```
The image depicts a serene landscape featuring a wooden pathway that extends into the distance, cutting through a lush green field.
The field is filled with tall grasses and a variety of low-lying shrubs and bushes.
In the background, there are scattered trees, and the sky above is a clear blue with some wispy clouds.
The scene exudes a calm, peaceful, and natural atmosphere.
```

Let's do the same with `claude-3-sonnet`, with a different image this time:

<p align="left">
    <img width={384} src="/images/ant.jpg" alt="Ant Image" />
</p>

```python
import unify
client = unify.Unify("claude-3-sonnet@anthropic")
response = client.generate(
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What’s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response)
```

We get something like the following:

```
The image shows a close-up view of an ant. The ant appears to be a large black carpenter ant species.
The ant is standing on a light-colored surface, with its mandibles open and antennae extended.
The image captures intricate details of the ant's body segments and legs.
The background has a reddish-brown hue, creating a striking contrast against the dark coloration of the ant.
This macro photography shot highlights the remarkable structure and form of this small insect in great detail.
```

## Platform Arguments

In addition to the unified arguments, we also accept other arguments specific to our platform,
referred to as **Platform Arguments**
- `signature` specifying how the API was called (Unify Python Client, NodeJS client, Console etc.)
- `use_custom_keys` specifying whether to use custom keys or the unified keys with the provider.
- `tags`: to mark a prompt with string-metadata which can be used for filtering later on.
- `drop_params`: in case arguments passed aren't supported by certain providers, uses [this](https://docs.litellm.ai/docs/completion/drop_params)
- `region`: the region where the endpoint is accessed, only relevant for certain providers like `vertex-ai` and `aws-bedrock`.

## Passthrough Arguments

The *passthrough* arguments are not handled by Unify at all, they are *passed through*
directly to the backend provider, without any modification.

There are three types of passthrough arguments:

1) extra headers, which are passed to the `curl` request like so:
    ```shell
    curl --request POST \
    --url https://api.unify.ai/v0/chat/completions \
    --header "<header_name>: <value>"
    ...
    ```

2) extra query parameters, which are passed to the `curl` request like so:
    ```shell
    curl --request POST \
    --url https://api.unify.ai/v0/chat/completions?<parameter>=<value> \
    ...
    ```

3) extra json properties, which are passed to the `curl` request like so:
    ```shell
    curl --request POST \
    --url https://api.unify.ai/v0/chat/completions \
    --data '{
    "<key>": "<value>"
    ...
    ```

In the Python client, these extra arguments are handled by the `extra_headers` argument, `extra_query` argument,
and direct `**kwargs` of the [generate function](https://docs.unify.ai/python/chat/clients/base#generate), respectively.

### Anthropic Headers Example

Features supported by providers outside of the OpenAI standard are sometimes released
as beta features, which can be accessed via specific headers, as explained in
[this tweet](https://x.com/alexalbert__/status/1812921642143900036) from Anthropic.

These headers can be queried directly from the Unify API like so:

```shell
curl --request POST \
  --url 'https://api.unify.ai/v0/chat/completions' \
  --header 'Authorization: Bearer $UNIFY_KEY' \
  --header 'Content-Type: application/json' \
  --header 'anthropic-beta: max-tokens-3-5-sonnet-2024-07-15' \
  --data '{
    "model": "claude-3.5-sonnet@anthropic",
    "messages": [
        {
            "content": "Tell me a joke",
            "role": "user"
        }
    ]
}'
```

Again, this can also be done in the Unify Python SDK as follows:

```python
client = unify.Unify("claude-3-haiku@anthropic")
client.generate("hello world!", extra_headers={"anthropic-beta": "max-tokens-3-5-sonnet-2024-07-15"})
```

{/* ToDo: add an example for a query parameter, again with both curl + python examples */}

## Python SDK

All of these arguments (both unified + platform arguments) are explicitly mirrored in the
[generate](https://docs.unify.ai/python/chat/clients/base#generate) function of the
[Unify](https://docs.unify.ai/python/chat/clients/uni_llm#unify) client and
[AsyncUnify](https://docs.unify.ai/python/chat/clients/uni_llm#asyncunify) client
in the Python SDK.

If you believe one of these arguments *could* be supported by a certain model or provider, but is not currently
supported, then feel free to let us know [on discord](https://discord.com/invite/sXyFF8tDtm)
and we'll get it supported as soon as possible! ⚡