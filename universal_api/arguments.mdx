---
title: 'Arguments'
---

With so many LLMs and providers constantly coming onto the scene, each of these is
increasingly striving to provide unique value to end users, and this means that there
are often diverging features offered behind the API.

For example, some models support RAG directly in the API,
others support function calling, tool use, image processing, audio,
structured output (such as json mode),
and many other increasingly complex modes of operation.

We *could* adopt a design for our universal API where we only support the lowest common
denominator across all of the APIs. However, this would necessarily leave out many of
the most exciting bleeding edge features, limiting the utility of
our API for more forward-thinking applications.

Similarly, we *could* try to create a universal interface to the full superset of features
across all providers, ensuring that the input-output behaviour is consistent regardless
of the backend provider selected. This would require a huge amount of ongoing
maintenance to keep pace with the fast-chaging API specs, and the wrong choice of
abstraction for the unification effort could break compatibility across APIs.

We have instead opted for a compromise with our API, where we support both *unified*
arguments and *passthrough* arguments, as explained below.

### Unified Arguments

The *unified* arguments are all of those explicitly listed in our
[chat completions endpoint](https://docs.unify.ai/api-reference/querying_llms/get_completions).
At the time of writing, this is:

- model
- messages
- temperature
- stream
- max_tokens
- frequency_penalty
- logit_bias
- logprobs
- top_logprobs
- n
- presence_penalty
- response_format
- seed
- stop
- top_p
- tools
- tool_choice
- user
- signature
- use_custom_keys
- tags

Most of these are taken directly from the
[OpenAI Standard](https://platform.openai.com/docs/api-reference/chat/create),
with the exceptions being:
- `signature` specifying how the API was called (Unify Python Client, NodeJS client, Console etc.) #TODO: should we even expose this
- `use_custom_keys` specifying whether to use custom keys or the unified keys with the provider.
- `tags`: to mark a prompt with string-metadata which can be used for filtering later on.

These *unified* arguments are also all mirrored in the
[generate](https://docs.unify.ai/python/clients#generate) function of the
[Unify](https://docs.unify.ai/python/clients#unify) client and
[AsyncUnify](https://docs.unify.ai/python/clients#asyncunify) client
in the Python SDK.

These arguments are all handled consistently regardless of the backend provider selected.
This makes it very easy for users to simply switch out the backend model and provider,
without needing to change anything else in the code.

For example, we can run this query based on the OpenAI arguments
`arg_a` # ToDo
and
`arg_b`, # ToDo
despite
`provider` # ToDo
not
supporting
`a` # ToDo
or
`b`. # ToDo

CODE # ToDo

The same is true via the Python SDK:

CODE # ToDo


### Tool use example

OpenAI and Anthropic have different interfaces for tool use.
Since we adhere to the OpenAI standard, we accept tools as specified by the OpenAI standard, and convert the format so that they work with Anthropic models.

#TODO: make these work in the python SDK (as tool-use doesn't return "chat" messages)

This is the default function calling example from OpenAI, working with an Anthropic model:

```
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
            },
        },
    }
]

client = unify.Unify("claude-3.5-sonnet@aws-bedrock")
client.generate("What is the current temperature of New York, San Francisco and Chicago?", tools=tools, tool_choice="auto")
```


### Passthrough Arguments

The *passthrough* arguments are not handled by Unify at all, they are *passed through*
directly to the backend provider, without any modification.

This is what happens for **any extra arguments** passed to the `--data` argument of the
`curl` request.

For example, anthropic supports the extra header `` for Y.
We can make use of this provider-specific header by simply selecting the correct
provider in the model argument, and then passing the argument in the --data field as
follows:

CODE

As indicated by the name, we do not handle these extra arguments in any way, and if you
do not make correct use of the argument, or you pass an invalid argument for a provider,
then we simply return the provider-specific error message to you,
with no other intervention.

Similarly, these arguments can also be used via the Python SDK as follows:

CODE




#### Anthropic-only example

Anthropic exposes the `top_k` argument, which isn't provided by OpenAI.
If you include this argument, it will be sent straight to the model.
If you send this argument to a provider that does not support `top_k`, you may get an error.

```
curl --request POST \
  --url 'https://api.unify.ai/v0/chat/completions' \
  --header 'Authorization: Bearer <UNIFY_KEY>' \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "claude-3.5-sonnet@anthropic",
    "messages": [
        {
            "content": "Tell me a joke",
            "role": "user"
        }
    ],
    "top_k": 5,
    "max_tokens": 1024,
}'
```

#TODO: Actually make this work in the python package.

This can be done in the Unify Python SDK as follows:

```
client = unify.Unify("claude-3-haiku@anthropic")
client.generate("hello world!", top_k=5)
```

#### Multi-Modal Queries

The *passthrough* approach means that Unify also supports multi-modal inputs,
and indeed supports anything which any of the providers support.
Below are a few examples of multi-modal queries, making use of passthrough arguments:

EXAMPLES

Image from a URL with OpenAI

For example, let's use `gpt-4o` to work out what's in this picture:
<img
  src="/images/nature.png"
  alt="Nature image"
/>

```
import unify
client = unify.Unify("gpt-4o@openai")
response = client.generate(
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What’s in this image?"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response)
```


Image from a file with Anthropic

```
import unify
import base64
import httpx

image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
image_media_type = "image/jpeg"
image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")

client = unify.Unify("claude-3-sonnet@anthropic")
response = client.generate(
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What’s in this image?"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": image_media_type,
                        "data": image_data,
                    },
                },
            ],
        }
    ],
    max_tokens=300,
)
print(response)
```