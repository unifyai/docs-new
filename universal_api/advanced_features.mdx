---
title: 'Advanced features'
---

### Custom Endpoints

If you have a custom model which is deployed as an endpoint on (for example a fine-tuned
model with OpenAI or Together AI) you can
[add your own custom endpoint](https://console.unify.ai/endpoints).

To create a custom endpoint, you need the Endpoint URL and the relevant API key. You can
query a custom endpoint using the model string `<endpoint-name>@custom`

### LLM Fallbacks

Sometimes individual providers have outages, which can disrupt live workflows in production.

To combat this, set a list of fallback models, so if one provider is down or fails for some
reason, the request will go to the next model on the list, and so on, until either the
request succeeds, or the end of the list is reached.

To specify the list of fallback models, use `->` between individual model tags, so the
model string becomes `model_a@provider_a->model_b@provider_b`.

There is no limit on the number of models that can be specified. The `model` field in the
response contains the model and provider that the request actually went to.
