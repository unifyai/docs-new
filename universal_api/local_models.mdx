---
title: 'Local Models'
---

Several recent tools have made it very easy to deploy SOTA LLMs locally.
This can help with security concerns, improve telemetry, and also save hugely on costs.
Thankfully, adding local models to your universal API is very easy,
either via our [custom endpoint API](<link-to-custom-endpoints-in-console>) interface,
or via a direct binding to the Python client (bypassing the need for HTTP requests).

Below, we show some end-to-end examples for deploying models locally
using some of the most popular libraries,
and we then show how these can be integrated into your unified stack.
Finally, we show how this can be done for any arbitrary local model,
making use of our open source unification layer.

## HuggingFace

## Ollama

## LLMWare

## llama.cpp

## Custom