---
title: 'Upload Evaluation'
api: 'POST /v0/evaluation'
---
Uploads evaluations, which can then be accessed using the `/v0/evaluation` endpoint.
If a custom prompt is specified, its fields will overwrite the corresponding fields
in each one of the evaluated prompts. If a response for a given prompt has already
been provided for the selected endpoint, during another evaluation, then this
response will be re-used during the current evaluation.

#### Authorizations

<ParamField header="Authorization" type="string" required="true">
  Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.
</ParamField>

#### Query Parameters

<ParamField query="evaluator" type="string"  default={"default_evaluator"}>
Name of the evaluator used.
</ParamField>

<ParamField query="default_prompt" type="string | null" >
Name of the default prompt used.
</ParamField>

<ParamField query="agent" type="string" required="true">
The agent to fetch the evaluation for, can be an endpoint string or an arbitrarily named client-side agent. If `None`, returns all available evaluations for the dataset and evaluator pair.
</ParamField>

#### Body

<ParamField body="evaluations" type="binary" required="true">
The evaluation results to upload. The file must be in JSONL format and the prompts must match the order of the `dataset`. Each entry should include `datum_id` and `score` keys, with `score` being a float between 0 and 1. The evaluation corresponding to the `evaluator` must have `client_side=True`.
</ParamField>

<RequestExample>

```bash cURL
curl --request POST \
  --url 'https://api.unify.ai/v0/evaluation?evaluator=eval1&default_prompt=default_prompt1&agent=gpt-4o-mini@openai' \
  --header "Authorization: Bearer $UNIFY_KEY"
  --header 'Content-Type: multipart/form-data' \
  --form 'evaluations=@client_scores.jsonl'
```

```python Python
import requests

url = "https://api.unify.ai/v0/evaluation?evaluator=eval1&default_prompt=default_prompt1&agent=gpt-4o-mini@openai"

headers = {"Authorization": "Bearer <token>"}

data = {}

file_path = "client_scores.jsonl"
with open(file_path, "rb") as file:
   files = {"file": file}
   response = requests.request("POST", url, files=files, data=data, headers=headers)

print(response.text)
```

</RequestExample>
<ResponseExample>

```json 200
{
    "info": "Dataset evaluation successfully Uploaded! "
}
```

```json 404
{
    "detail": "This dataset does not exist!"
}
```

```json 422
{
    "detail": [
        {
            "loc": [
                "string"
            ],
            "msg": "string",
            "type": "string"
        }
    ]
}
```

</ResponseExample>
