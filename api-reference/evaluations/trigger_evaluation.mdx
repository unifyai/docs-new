---
title: 'Trigger Evaluation'
api: 'POST /v0/evaluation/trigger'
---
Uses the named `evaluator` to trigger an evaluation of quality scores for the
selected LLM `endpoint` on the selected `dataset`, or selected `prompts` (by
datum_id). Once the evaluation has finished, you can access the scores using the
`/v0/evaluation` endpoint. If a custom prompt is specified, its fields will
overwrite the corresponding fields in each one of the evaluated prompts. If a
response for a given prompt has already been provided for the selected endpoint,
during another evaluation, then this response will be re-used during the current
evaluation.

#### Authorizations

<ParamField header="Authorization" type="string" required="true">
  Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.
</ParamField>

#### Query Parameters

<ParamField query="evaluator" type="string"  default={"default_evaluator"}>
Name of the evaluator to use. If not specified, 'default_evaluator' will be used.
</ParamField>

<ParamField query="default_prompt" type="string" >
Name of the default prompt to use.
</ParamField>

<ParamField query="dataset" type="string | null" >
Name of the uploaded dataset to evaluate. Must pass exactly one of `dataset`, `prompts`.
</ParamField>

<ParamField query="prompts" type="string | null" >
Specify the prompts to evaluate. Pass a string of comma separated integers. Must pass exactly one of `dataset`, `prompts`.
</ParamField>

<ParamField query="agent" type="string" required="true">
The agent to fetch the evaluation for, can be an endpoint string or an arbitrarily named client-side agent. If `None`, returns all available evaluations for the dataset and evaluator pair.
</ParamField>

<RequestExample>

```bash cURL
curl --request POST \
  --url 'https://api.unify.ai/v0/evaluation/trigger?evaluator=eval1&default_prompt=default_prompt1&dataset=dataset1&prompts=34,89,127&agent=gpt-4o-mini@openai' \
  --header "Authorization: Bearer $UNIFY_KEY"
```

```python Python
import requests

url = "https://api.unify.ai/v0/evaluation/trigger?evaluator=eval1&default_prompt=default_prompt1&dataset=dataset1&prompts=34,89,127&agent=gpt-4o-mini@openai"

headers = {"Authorization": "Bearer <token>"}

response = requests.request("POST", url, headers=headers)

print(response.text)
```

</RequestExample>
<ResponseExample>

```json 200
{
    "info": "Dataset evaluation started! You will receive an email soon!"
}
```

```json 400
{
    "detail": "Invalid input. Couldn't find endpoints [model_1@endpoint_1, model_2@endpoint_2]."
}
```

```json 404
{
    "detail": "This dataset does not exist!"
}
```

```json 422
{
    "detail": [
        {
            "loc": [
                "string"
            ],
            "msg": "string",
            "type": "string"
        }
    ]
}
```

</ResponseExample>
