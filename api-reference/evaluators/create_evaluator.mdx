---
title: 'Create Evaluator'
api: 'POST /v0/evaluator'
---
Create a re-usable, named evaluator, and adds this to your account. This can be used
to trigger an evaluation via `POST` requests to the `/evaluation` endpoint.

#### Authorizations

<ParamField header="Authorization" type="string" required="true">
  Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.
</ParamField>

#### Body

<ParamField body="name" type="string" required="true">
A unique, user-defined name used when referencing and triggering the evaluation.
</ParamField>

<ParamField body="description" type="string | null" >
Description for the purpose of this evaluator. In the case of Evaluators defined in the Python client, this is the docstring by default, if it exists.
</ParamField>

<ParamField body="judge_prompt" type="string | any | null" >
An optional custom system prompt to provide specific instructions to the judge on how to score the answers.
</ParamField>

<ParamField body="prompt_parser" type="object | null"  default={{"user_message": ["messages", -1, "content"]}}>
Dict with str  keys and indexing logic values. Default value of: `{'user_message': ['messages', -1, 'content']}` This is used by the system prompt to replace each key {some_key} with prompt.dict()\<indexing_logic\> The default value will replace all occurances of {user_message} with prompt.dict()['messages'][-1]['content'] in the judge prompt.
</ParamField>

<ParamField body="response_parser" type="object | null"  default={{"assistant_message": ["message", "content"]}}>
Dict with str  keys and indexing logic values. Default value of: `{'assistant_response': ['message', 'content']}` This is used by the system prompt to replace each key {some_key} with response.dict()\<indexing_logic\> The default value will replace all occurances of {assisntant_message} with response.dict()['message']['content'] in the judge prompt.
</ParamField>

<ParamField body="extra_parser" type="object | null" >
Dict with str  keys and indexing logic values. Default value of: `None` This is used by the system prompt to replace each key {some_key} with datum.dict()\<indexing_logic\> which can be used to index into extra fields stored within each item in the dataset.
</ParamField>

<ParamField body="class_config" type="array | null" >
If set, describes the list of classifications that the LLM judge uses to
            score each prompt. For example:
```
[{"label": "Excellent", "score": 1.0},
{"label": "Good", "score": 0.5},
{"label": "Bad", "score": 0.0}]
```

</ParamField>

<ParamField body="judge_models" type="string | array"  default={"claude-3.5-sonnet@aws-bedrock"}>
Specifies the LLM(s) to be used as the judge. This can be a string containing a single model name or a list of model names.
</ParamField>

<ParamField body="client_side" type="boolean"  default={false}>
Indicates whether evaluations are performed on the client-side. If `True`, the LLM judge is bypassed, and results are uploaded via the `trigger` endpoint.
</ParamField>

<RequestExample>

```bash cURL
curl --request POST \
  --url 'https://api.unify.ai/v0/evaluator' \
  --header "Authorization: Bearer $UNIFY_KEY" \
  --header 'Content-Type: application/json' \
  --data '{
    "name": "eval1",
    "description": "The clarity of the generated summary",
    "judge_models": "claude-3.5-sonnet@aws-bedrock"
}'
```

```python Python
import requests

url = "https://api.unify.ai/v0/evaluator"

headers = {"Authorization": "Bearer <token>"}

json_input = {"name": "eval1", "description": "The clarity of the generated summary", "judge_models": "claude-3.5-sonnet@aws-bedrock"}

response = requests.request("POST", url, json=json_input, headers=headers)

print(response.text)
```

</RequestExample>
<ResponseExample>

```json 200
{
    "info": "Evaluator created successfully!"
}
```

```json 422
{
    "detail": [
        {
            "loc": [
                "string"
            ],
            "msg": "string",
            "type": "string"
        }
    ]
}
```

</ResponseExample>
