---
title: 'Post Inference'
api: 'POST /v0/inference'
---
DEPRECATION WARNING: This endpoint is deprecated and will be removed. Instead,
use the /chat/completions endpoint for LLM inference.

#### Authorizations

<ParamField header="Authorization" type="string" required="true">
  Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.
</ParamField>

#### Body

<ParamField body="model" type="string" required="true">
None
</ParamField>

<ParamField body="provider" type="string" required="true">
None
</ParamField>

<ParamField body="arguments" type="object" required="true">
None
</ParamField>

<ParamField body="signature" type="string | null" >
None
</ParamField>

<RequestExample>

```bash cURL
curl --request POST \
  --url 'https://api.unify.ai/v0/inference' \
  --header 'Authorization: Bearer <UNIFY_KEY>' \
  --header 'Content-Type: application/json' \
  --data '{}'
```

```python Python
import requests

url = "https://api.unify.ai/v0/inference"

headers = {"Authorization": "Bearer <token>"}

json_input = {}

response = requests.request("POST", url, json=json_input, headers=headers)

print(response.text)
```

</RequestExample>
<ResponseExample>

```json 200
{}
```

```json 422
{
    "detail": [
        {
            "loc": [
                "string"
            ],
            "msg": "string",
            "type": "string"
        }
    ]
}
```

</ResponseExample>
