---
title: 'Routing Syntax'
---

In the previous section, we introduced *fallbacks*, which enable you to specify what to
do in the case of outages.

However, you can also specify routing logic to directly *maximize LLM performance*,
whether that be be improving *cost*, *quality* or *speed*. First, we outline the
[Base Metrics](#base-metrics) which can be optimized over, we then introduce the concept
of [Meta Providers](#meta-providers) (models used with whatever provider optimizes a
given metric, based on the latest available benchmark data),
we then introduce [Thresholds](#thresholds) (specifying the permitted bounds for certain metrics),
we then explain how [Models, Providers, and Endpoints](#models-providers-and-endpoints)
can be specified, then how [Default Prompts](#default-prompts) can be specified, and
finally we introduce [Custom Metrics](#custom-metrics), which enable you to customize
exactly how cost, quality and speed should be balanced when routing.

## Base Metrics

The *base metrics* and their permitted syntax when making chat completion queries,
are as follows:

- **Quality** (The quality of the generated response): `quality, q`
- **Time to First Token**: `time-to-first-token, ttft, t`
- **Inter Token Latency**: `inter-token-latency, itl, i`
- **Cost** ($ per million tokens based on OpenHermes input/output ratio): `cost, c`
- **Input Cost** ($ per million input tokens): `input-cost, ic`
- **Output Cost** ($ per million output tokens): `output-cost, oc`

## Meta Providers

Any of these *base metrics* can be specified in place of a *provider*,
when making chat completion requests. For example, lets assume we want to deploy
`claude-3-opus` with whichever provider has the fastest `inter-token-latency`,
based on the latest benchmark data. We can specify the following:

```shell
curl -X 'POST' \
    'https://api.unify.ai/v0/chat/completions' \
    -H 'Authorization: Bearer $UNIFY_KEY' \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "claude-3-opus@inter-token-latency",
        "messages": [{"role": "user", "content": "Hello."}]
    }'
```

To be more explicit, the prepending keywords `highest-` and `lowest-` can be specified.
When the keyword is omitted, it's assumed that the user wants the "best" option, which
is `highest-` for quality and `lowest-` for all other metrics.
The example above is equivalent to `claude-3-opus@lowest-inter-token-latency`.

## Thresholds

You can specify upper and/or lower bounds for any of these metrics using `<`, `>`, `<=`
and `>=` and separating each specification with `|` in the command. For example, the
following command will find the `llama-3.1-405b-chat` provider with the lowest inter
token latency (the fastest), provided that the endpoint is not more expensive that $5
per million tokens:

```shell
llama-3.1-405b-chat@inter-token-latency|c<5
```

The following will find the `llama-3.1-70b-chat` provider with the highest `quality`
(see explanation for how this is computed below).
Differences such as quantization can result in *different quality* for the *same model*.
The endpoint is also constrained to not be more expensive than `$0.8` per million tokens
in the *input*, not be more expensive than `$0.6` per million tokens in the *output*,
have an inter token latency faster than `20ms`, but not faster than `1ms`:

```shell
llama-3.1-70b-chat@quality|input-cost<=0.8|output-cost<=0.6|1<itl<20
```

As explained above, any of the aliases defined above can be used interchangeably,
such as `i`, `itl` or `inter-token-latency` for the inter token latency.

## Model Routing

All of the examples above have assumed that there is a *single model* chosen,
and we're then just optimizing over the *provider* for that chosen model.

However, we can also include the model within the routing decision. To do so, we simply
specify the model as `router`. For example, the following will find the highest quality
endpoint (among *all* supported endpoints!), which is cheaper than `$0.8` per million
input tokens, cheaper than `$0.6` per million output tokens, and has an inter token
latency faster than `20ms`:

```shell
router@quality|input-cost<0.8|output-cost<0.6|itl<20
```

## Custom Metrics

So far, we have only considered cases where we optimize for a *single base metric*, and
then constrain the search space for other metrics. However, you can also specify your
own *custom metric*, as a linear combination of the base metrics:

```
custom_metric = q_f*q - i_f*i - t_f*t - c_f*c
```

In words, this metric takes factors `{}_f` for each of the four base metrics:
`quality`, `inter-token-latency`, `time-to-first-token` and `cost`, and then computes
their linear combination, with the objective of *maximizing* `custom_metric`.

With this form factor, controlling the custom metric is reduced to specifying the linear
factors `{}_f`. These linear factors can be specified as follows:

```
router@q:1|i:0.5|t:2|c:0.7
```

Factors which are left unspecified are assumed to be zero (their contribution to the
custom metric is ignored). For example, the following as aliases:

```
router@q:1|i:0.5 == router@q:1|i:0.5|t:0|c:0
```

As before, any variant of the metric can be used to specify the factors:

```
router@q:1|i:0.5 == router@quality:1|inter-token-latency:0.5
```

The Meta Providers explained above are actually aliases to this same logic,
and under the hood they resolve to these factors:

```
llama-3.1-70b-chat@quality == llama-3.1-70b-chat@q:1 == llama-3.1-70b-chat@q:1|i:0|t:0|c:0
llama-3.1-70b-chat@itl == llama-3.1-70b-chat@i:1 == llama-3.1-70b-chat@q:0|i:1|t:0|c:0
```

For this reason, meta-providers cannot be specified alongside these factors.
Meta providers are basically convenient one-liners to specify *all four factors*.

Note that the units are **not normalized**. It's down to **you** to reason about
sensible values for the factors, based on the respective units:
`cost` in $ per M tokens, `itl` in ms, `ttft` in ms, and `quality` normalized to `0-1`.

When experimenting, you can retrieve the four metrics for any endpoint as follows:

```shell
curl -X 'GET' \
    'https://api.unify.ai/v0/router/metric?endpoint=llama-3.1-405b-chat@together-ai' \
    -H 'Authorization: Bearer $UNIFY_KEY'
```

This will return a dictionary like below:

```
{
    "quality": 0.78
    "inter-token-latency": 15.67
    "time-to-first-token": 752.39
    "cost": 0.73
}
```

The custom metric can then be reasoned with very easily on the client side,
by applying any linear combination as desired.


## The Search Space

All of the examples above have assumed that *all compatible* models and/or providers are
included in the "search space" when making routing decisions. However,
you might want to limit the search space.
This can be achieved with the `models`, `providers`, `endpoints`, `skip_models`,
`skip_providers`, and `skip_endpoints` keywords.
Again, this is simply specified within the same chain, separated by `|`.

The following finds the fastest (lowest `itl`) endpoint for `llama-3.1-405b-chat`,
among the providers: `groq`, `fireworks-ai` and `together-ai`:

```shell
llama-3.1-405b-chat@itl|providers:groq,fireworks-ai,together-ai
```

The following finds the fastest `llama-3.1-405b-chat` endpoint among *all supported*
providers, *excluding* (skipping): `azure-ai` and `aws-bedrock`:

```shell
llama-3.1-405b-chat@itl|skip_providers:azure-ai,aws-bedrock
```

These keywords are also compatible when routing across models. The following optimizes
a custom metric, whilst only considering the models `gpt-4o`, `o1-preview` and
`claude-3-sonnet`:

```shell
router@q:1|i:0.5|models:gpt-4o,o1-preview,claude-3-sonnet
```

Providers and models can also *both* be limited. The following only considers endpoints
which are within the set of models **and** providers. It does not find the *superset* of
endpoints matching the models and providers, but rather the *subset*. For example,
the endpoints `claude-3-haiku@vertex-ai` and `llama-3-8b-chat@aws-bedrock` are **not**
included in the search space for the query below, but the endpoint
`claude-3-haiku@anthropic` **is** included:

```shell
router@q:1|i:0.5|models:claude-3-haiku,claude-3-sonnet,claude-3-opus|providers:anthropic,aws-bedrock
```

Note that specifying a single model in `models` using the `router` keyword is
functionally equivalent to specifying that model on the left-hand-side:

```
router@itl|models:llama-3.1-405b-chat == llama-3.1-405b-chat@itl
```

## Quality Prediction

## Default Prompts
