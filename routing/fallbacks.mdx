---
title: 'Fallbacks'
---

Sometimes individual providers have outages,
which can disrupt live workflows in production.

To combat this, you can set a list of fallback providers, models, endpoints, or even
entire fallback queries (if different models require unique system messages,
temperature etc.). Therefore, if one provider goes down or fails to respond for some
reason, the request will go to the next model on the list, and so on, until either the
request succeeds, or the end of the list is reached.

## Fallback Models, Providers and Endpoints

Lets assume you're deploying `claude-3-opus` for a production application, where
downtime would be detrimental for your users. You can set `anthropic` as your default
provider, with `aws-bedrock` as your *fallback provider* if Anthropic goes down or there
are rate limit issues, like so:

```shell
curl -X 'POST' \
    'https://api.unify.ai/v0/chat/completions' \
    -H 'Authorization: Bearer $UNIFY_KEY' \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "claude-3-opus@anthropic->aws-bedrock",
        "messages": [{"role": "user", "content": "Hello."}]
    }'
```

The same logic can also be applied to different models with the same provider,
for example maybe you're deploying `gemini-1.5-pro` on `vertex-ai`, but you hit
occasional rate limit issues, in which you want to fallback to `gemini-1.5-flash` as
your *fallback model*, again on `vertex-ai`. This can be specified like so:

```shell
curl -X 'POST' \
    'https://api.unify.ai/v0/chat/completions' \
    -H 'Authorization: Bearer $UNIFY_KEY' \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "gemini-1.5-pro->gemini-1.5-flash@vertex-ai",
        "messages": [{"role": "user", "content": "Hello."}]
    }'
```

Finally, you can also specify fallbacks for the *entire endpoint*.
For example, maybe you don't want to keep your users waiting, and so if the first
attempt to a larger model fails, then you just want to give them a response quickly:

```shell
curl -X 'POST' \
    'https://api.unify.ai/v0/chat/completions' \
    -H 'Authorization: Bearer $UNIFY_KEY' \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "llama-3.1-405b-chat@together-ai->llama-3.1-70b-chat@groq",
        "messages": [{"role": "user", "content": "Hello."}]
    }'
```

The Fallback logic can be composed arbitrarily across providers, models, and endpoints.
For example, the following will attempt `llama-3.1-405b-chat` for a variety of providers
before falling back to `llama-3.1-70b-chat@groq` if they all fail:

```shell
curl -X 'POST' \
    'https://api.unify.ai/v0/chat/completions' \
    -H 'Authorization: Bearer $UNIFY_KEY' \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "llama-3.1-405b-chat@together-ai->fireworks-ai->vertex-ai->llama-3.1-70b-chat@groq",
        "messages": [{"role": "user", "content": "Hello."}]
    }'
```

This will attempt `gemini-1.5-pro` and `gemini-1.5-flash` with `vertex-ai`,
then attempt `llama-3.1-405b-chat` with `together-ai` and `fireworks-ai` if they fail,
and then finally attempt `llama-3.1-70b-chat@groq`:

```shell
curl -X 'POST' \
    'https://api.unify.ai/v0/chat/completions' \
    -H 'Authorization: Bearer $UNIFY_KEY' \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "gemini-1.5-pro->gemini-1.5-flash@vertex-ai->llama-3.1-405b-chat@together-ai->fireworks-ai->vertex-ai->llama-3.1-70b-chat@groq",
        "messages": [{"role": "user", "content": "Hello."}]
    }'
```