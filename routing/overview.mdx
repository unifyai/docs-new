---
title: 'Overview'
---

LLM routing allows you to be flexible about which model, provider and endpoint handles
each prompt. Flexibility can be advantageous for several reasons:

1. Small models are (in general) faster and cheaper, whereas bigger models are more capable.
2. Tasks often exhibit a range of difficulties, suitable for varying LLM capability
3. Different providers have different latencies, and these **change** over time.
4. New models come out every week, each having different strengths and weaknesses.

<p align="center"><iframe width="672" height="378" src="https://www.youtube.com/embed/15wgxK1Cw0E?si=JnrC0yiU5J4ju5Yk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>

LLM routing provides:

- Faster and cheaper responses when a smaller model is capable of answering
- Continuous improvement: ‘riding the wave’ of new model releases
- Ability to maximise throughput or minimise latency based on live runtime statistics
- Reliability via fallbacks, if providers go down or latency limits are hit
