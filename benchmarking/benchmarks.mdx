---
title: 'Benchmarks'
---

Having triggered evaluations, we can now see how different endpoints compare in terms of
their output quality. However, what about the cost and speed of these endpoints?

NOTE: We refer to the LLM output quality as "evaluations"
(it's especially involved and non-trivial, thus the more active word evaluation).
We refer to the *complete picture* of quality, cost and speed as "benchmarking",
as this is the process of extracting the tabular speed and cost data (less involved),
and comparing everything (more focus on tabular data and comparisons,
thus "benchmarking").
Whether or not this distinction resonates, it's the terminology we use!

### Procedural Benchmarking

Explain the full shebang.

### Custom Endpoints

Custom endpoints ([local models]() or [fine-tuned models](), for example), will not
include any speed or cost data by default. This means that the co-ordinates will not be
displayed on the [dashboard]() (see [next section]()). However, the evaluations will
still be displayed on the right hand table.

If you would like to compare your custom endpoints in terms of speed and cost on the
dashboard, then you simply need to publish speed and cost values to the `X` endpoint,
as follows:

CODE

The timestamp of the submission is automatically detected, and the data can be streamed
to this endpoint in a recurring basis if so desired, similar to how we do it for the
public endpoints. If the time of submission does not align with the time of measurement,
then the timestamp can be provided explicitly via the `x` argument, as follows:

CODE

In the next section, we explain how to use the [dashboard]() to view the benchmarking
results in a clear and intuitive way!