---
title: 'Evaluations'
---

Once our [evaluators](evaluators) are defined, we can then actually *use* these evaluators to
evaluate the performance of different endpoints on different prompt datasets.

## Trigger Evaluations

To trigger an LLM evaluation using a pre-configured [LLM evaluator](), you simply need
to specify the LLM endpoint, the dataset, and the pre-configured evaluator you would like to
use, as follows:

```python
client.evaluation(
    evaluator="computer_science_judge",
    dataset="computer_science_challenges",
    endpoint="llama-3-70b-chat@aws-bedrock",
)
```

You will receive en email once the evaluation is finished.
We will explain how to visualize the results of your evaluations in the next section.

## Checking Evaluations

You can check the status of an evaluation using the endpoint X, as follows:

```python
status = client.evaluation_status(
    evaluator="computer_science_judge",
    dataset="computer_science_challenges",
    endpoint="llama-3-70b-chat@aws-bedrock",
)
print(status)
```

You can get the aggregated scores across the dataset as follows:

```python
scores = client.evaluation_scores(
    evaluator="computer_science_judge",
    dataset="computer_science_challenges",
)
print(scores)
```

You can also get more granular results, with per-prompt scores by passing `per_prompt=True`.

```python
per_prompt_scores = client.evaluation_scores(
    evaluator="computer_science_judge",
    dataset="computer_science_challenges",
    per_prompt="True",
)
print(per_prompt_scores)
```

{/* ToDo in API */}
If the dataset has been updated since the evaluation was run, then the status `this`
will be shown when making the query (see [Partial Evaluations]() below).

## Custom Evaluators

If you do not need an LLM as a judge for your evaluation,
then you can perform the evaluation on the client side.

For example, [as before](), let's assume our LLM is generating Python code,
and we want to verify that the output is valid Python code.

We use the same custom Python Evaluator class we previously defined:

CODE

We can then trigger the custom eval on the client side as follows.

CODE

Similarly, we can use our keyword checker evaluator:

CODE

Triggering this on some geography prompts:

CODE

The code will run locally, and once the evaluation has been performed for all prompts
in the dataset, the results will be uploaded via the X endpoint, using the Y argument.

### Client side scores

If you want to submit evaluations that you obtained locally, you can via the `/evaluator` endpoint, by passing
`client_side_scores` as the file.

The file should be in JSONL format, with entries having `prompt` and `score` keys:

```
{"prompt": "Write Hello World in C", "score": 1.0}
{"prompt": "Write a travelling salesman algorithm in Rust", "score": 0.2}
```
The prompts must be the same prompts as the ones from the `dataset`.
The evaluator must be created with `client_side=True`.

```python
client.evaluation(
    evaluator="computer_science_judge",
    dataset="computer_science_challenges",
    endpoint="llama-3-70b-chat@aws-bedrock",
    client_side_scores="/path/to/scores.jsonl"
)
```

## Partial Evaluations

As mentioned [here](), datasets can be appended to at any point in time, which can make
previously run evaluations "out of date" with respect to the full dataset
(including the most recent prompts).

As explained above, when querying the X endpoint with an out-of-sync dataset,
the results will be returned, but the status will show as `this`.