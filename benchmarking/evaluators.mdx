---
title: 'Evaluators'
---


On a high level, an evaluator is *basically* any function which receives a prompt,
receives optional extra fields corresponding to the prompt, receives a *response* to
the prompt, and then returns a score evaluating the response, that's it!

There is a bit more nuance to the exact types used in the implementation,
but conceptually, this is all an evaluator is.
Let's dive in and create a simple evaluator! 🧑‍💻

## Creation

To begin with, let's assume we have the following dataset to evaluate:

```python
import unify
from typing import Union, Type, Dict
system_msg = "Answer the following maths question, " \
             "returning only the numeric answer, and nothing else."
dataset = unify.Dataset(
    [unify.Prompt(q, system_message=system_msg) for q in ["1 + 3", "4 + 7", "6 + 5"]]
)
```

Before creating an `Evaluator`, we need to define the scoring system, giving several
different available scores between 0-1, each with their own *description*.
We do this by subclassing `unify.Score`, and overriding the abstract `config` property,
as follows:

```python
class Binary(unify.Score):

    @property
    def config(self) -> Dict[float, str]:
        return {
            0.: "incorrect",
            1.: "correct"
        }
```

An evaluator can then be created by subclassing `unify.Evaluator`,
and overriding the abstract `class_config` property and the abstract `_evaluate` method,
as follows:

```python
class MathsEvaluator(unify.Evaluator):

    @property
    def class_config(self) -> Type[unify.Score]:
        return Binary

    def _evaluate(self, prompt: str, response: str) -> bool:
        correct_answer = eval(prompt)
        try:
            response_int = int(
                "".join([c for c in response.split(" ")[-1] if c.isdigit()])
            )
            return correct_answer == response_int
        except ValueError:
            return False
```

Let's start by simply evaluating the first item in the dataset,
using OpenAI's `gpt-4o` as the model:

```python
client = unify.Unify("gpt-4o@openai")
evaluator = MathsEvaluator()
evaluation = evaluator.evaluate(
    dataset[0],
    client.generate(**dataset[0].prompt.dict()),
    client
)
```
```
Evaluation(
    prompt=Prompt("1 + 3"),
    response=ChatCompletion("4"),
    agent=Unify("gpt-4o@openai"),
    score=Score(1.0, "correct")
)
```

We can see that `gpt-4o` was able to get the answer correct!
Maybe AGI is around the corner after all 👀

Note that the output format is *very minimal*.
Again, as explain in both the [Prompt](https://docs.unify.ai/universal_api/prompts) and
[Responses](https://docs.unify.ai/universal_api/responses) sections,
by default all nested types in Unify collapse the nested structure *aggressively* in
order to minimize clutter in the terminal, and to enable quick and intuitive
visualization of the **important information**.

As explained in the prior sections, the full representation can be viewed as follows:

```python
print(evaluation.full_repr())
```
```
Evaluation(
    datum=Datum(
        prompt=Prompt(messages=[{'content': '1 + 3', 'role': 'user'}])
    ),
    response=ChatCompletion(
        id='',
        choices=[
            Choice(
                finish_reason='stop',
                index=0,
                logprobs=None,
                message=ChatCompletionMessage(
                    content='1 + 3 equals 4.',
                    refusal=None,
                    role='assistant',
                    function_call=None,
                    tool_calls=None
                )
            )
        ],
        created=0,
        model='',
        object='chat.completion'
    ),
    agent=Unify(endpoint=gpt-4o@openai),
    score=Score(score=(0.0, 'incorrect'))
)
```