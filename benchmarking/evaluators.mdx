---
title: 'Evaluators'
---

An evaluator is basically any function which takes in a prompt,
and any other corresponding fields which may be provided as part of a prompt dataset,
and returns a score between `0` and `1`. That's it!

Given the huge difficulty in evaluating LLM outputs at scale,
a very common pattern is to use LLMs themselves as a judge.

In the sections below, we explain how LLMs-as-a-judge can be flexibly configured for
different use cases.

In the next section, we'll explain how these *evaluators* can be used in *evaluations*
of datasets.

### LLM as a Judge

Evaluators are configured using the `/create_eval` endpoint, as follows:

```
url = "https://api.unify.ai/v0/evals/create"
headers = {"Authorization": f"Bearer {KEY}"}
params = {"eval_name": "my_first_eval"}
response = requests.post(url, json=params, headers=headers)
```

As per our [example](), let's assume we first want to choose an evaluator for
computer science. Which model should we select for this judge,
which will be judging the outputs of all LLMs we test out on this subject?

Accordingly to the [Scale AI Coding Leaderboard](https://scale.com/leaderboard/coding), `Claude 3.5 Sonnet` is the best model for coding.
Therefore, this is probably a good choice for our computer science judge.

What about English Literature? Apprently X is the best at Y, and so this is probably a
good choice for our English Literature, where creativity is important.

The judges can be configured via the `judge_models` parameter as follows:

```
url = "https://api.unify.ai/v0/evals/create"
headers = {"Authorization": f"Bearer {KEY}"}
params = {"eval_name": "computer_science_demo", "judge_models": "claude-3.5-sonnet@aws-bedrock"}
response = requests.post(url, json=params, headers=headers)
```

### LLM Jury

[Cohere showed that](https://arxiv.org/abs/2404.18796) LLM evals are generally less biased and more accurate when several
different models from different models are average pooled (a jury rather than a judge).

For our example, we might choose to use the models `Claude 3.5 Sonnet`, and `GPT-4o` for computer science,
and A, B and C for English Literature, again as per the [Scale AI X Leaderboard]().

The juries can be configured as follows:

```
url = "https://api.unify.ai/v0/evals/create"
headers = {"Authorization": f"Bearer {KEY}"}
params = {
    "eval_name": "computer_science_jury",
    "judge_models": ["claude-3.5-sonnet@aws-bedrock", "gpt-4o@openai"],
}
response = requests.post(url, json=params, headers=headers)
```

### Custom System Prompt

The default system prompt is as follows:

```
Please act as an impartial judge and evaluate the quality of the response provided by an assistant to the user question displayed below.
Your job is to evaluate how good the assistant's answer is.
Your evaluation should consider correctness and helpfulness. Identify any mistakes.
Be as objective as possible.
```

This is a very generic system prompt, which is designed to cast a very wide net and
produce *okay* results across any task.

However, it is certainly not optimized for Computer Science,
nor is it optimized for English literature.

We can create unique system prompts for these two subjects as follows,
based on some simple best practices for these domain areas:

```
computer_science_system_prompt = """
Please evaluate the quality of the student's code provided in response to the examination question below.
Your job is to evaluate how good the student's answer is.
Your evaluation should consider:
    - Code Correctness
    - Code Efficiency
    - Code Structure
    - General Readability and Style
Are there any edge cases that the code would break for? Is the code laid out neatly, and easy to reason about?
Be as objective as possible.
"""

url = "https://api.unify.ai/v0/evals/create"
headers = {"Authorization": f"Bearer {$UNIFY_API_KEY}"}
params = {
    "eval_name": "computer_science_judge",
    "judge_models": "claude-3.5-sonnet@aws-bedrock",
    "system_prompt": computer_science_system_prompt,
}
response = requests.post(url, json=params, headers=headers)
```

#TODO: English Literature system prompt.


### Custom judging classes

If you want to be really prescriptive about the criteria that responses are marked on, you can express this through the `class_config` parameter.

For example

```
class_config = [
    {"label": "Excellent", "score": 1.0, "description": "Correct code which is easy to read"},
    {"label": "Good", "score": 0.75, "description": "Correct code but structured badly"},
    {"label": "Good", "score": 0.5, "description": "Correct code but not using the most efficient method"},
    {"label": "Bad", "score": 0.0, "description": "Incorrect code that does not solve the problem"}
]
```


### Custom Classes

#TODO: this isn't implemented

Going further, we can also change the classification targets. This enables us to be much
more granular with the judging, and even measure several different properties for the
same subject.

For example, for english literature we might want to assess the creativity **and**
also assess the clarity. We could create two different evaluators accordingly:

CODE

For computer science, we might want to assess the clarity of the code comments **and**
the simplicity of the code.

CODE

### Custom Evaluators

There are many cases where you might not want to use an LLM to evaluate the output of
your LLM. For example, if you're generating code, you might want to test that the code
compiles (which certainly does not require an LLM to test). Similarly, you might want to
simply check that the output does or does not contain certain keywords, or you might
want to verify that the output follows are certain json format.

You can use the Python class `Evaluator` to create client-side logic for performing
evaluations. Let's assume our LLM will be generating Python code, we can create a custom
evaluator to check if the output is valid Python code like so:

CODE

Similarly, we can create a custom evaluator to check that the output does and does not
contain certain keywords as follows:

CODE

These custom evaluations are then all be handled on the client side.
There is no need to pre-configure these custom evaluators in the platform,
given that they do not run on the server.