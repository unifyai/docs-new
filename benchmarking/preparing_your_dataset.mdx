---
title: 'Preparing Your Dataset'
---

First create a dataset which is representative of the task you want to evaluate.
You will need a list of prompts, optionally including a reference, *gold-standard* answer. 
Datasets containing reference answers tend to get more accurate benchmarks.

The file itself should be in JSONL format, with one entry per line, as in the example below.

```json
{"prompt": "This is the first prompt", "ref_answer": "This is the first reference answer"}
{"prompt": "This is the second prompt", "ref_answer": "This is the second reference answer"}
```

Use at least 50 prompts to get the most accurate results. Currently there is an maximum 
limit of 500 prompts, for most tasks we donâ€™t tend to see much extra detail past ~250.
