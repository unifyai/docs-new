---
title: 'Overview'
---

When comparing LLMs, there is a constant tradeoff to make between quality, cost and 
latency. Stronger models are (in general) slower and more expensive - and sometimes 
overkill for the task at hand. Complicating matters further, new models are released 
weekly, each claiming to be state-of-the-art.

Benchmarking on your data lets you see how each of the different models perform on your 
task.

<img
  src="/images/benchmarks.png"
  alt="Benchmarks Image"
/>

You can compare how quality relates to cost and latency, with live stats pulled from 
our [runtime benchmarks](https://unify.ai/benchmarks).

When new models come out, simply re-run the benchmark to see how they perform on your task.
