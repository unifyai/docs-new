---
title: 'Overview'
---

When comparing LLMs, there is a constant tradeoff to make between quality, cost and 
latency. Stronger models are (in general) slower and more expensive - and sometimes 
overkill for the task at hand. Complicating matters further, new models are released 
weekly, each claiming to be state-of-the-art.

When fine-tuned models are added to the mix, the picture is even more complicated,
as these models typically outperform the more general foundation models in their
specific areas of fine-tuning.

Benchmarking on your prompt data lets you see how each of the different models perform
on your own task, in a visually intuitive manner.

<img
  src="/images/benchmarks.png"
  alt="Benchmarks Image"
/>